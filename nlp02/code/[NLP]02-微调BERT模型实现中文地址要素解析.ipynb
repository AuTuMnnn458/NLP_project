{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29656c5c",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "\n",
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ef07396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13808141",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/model/web/nlp02/data/train.txt', 'r', encoding='utf-8') as f:\n",
    "    txt = f.read().split('\\n\\n')\n",
    "    # 先使用\\n\\n划分出每个地址，然后在每个地址里再划分每个字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c8a463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5be895f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame([[t.split()[::2], t.split()[1::2]] for t in txt])\n",
    "train.columns = ['loc', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1da8960b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[浙, 江, 杭, 州, 市, 江, 干, 区, 九, 堡, 镇, 三, 村, 村, 一, 区]</td>\n",
       "      <td>[B-prov, E-prov, B-city, I-city, E-city, B-dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[浙, 江, 省, 温, 州, 市, 平, 阳, 县, 海, 西, 镇, 宋, 埠, 公, ...</td>\n",
       "      <td>[B-prov, I-prov, E-prov, B-city, I-city, E-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[浙, 江, 省, 余, 姚, 市, 模, 具, 城, 金, 型, 路, 0, 0, 0, ...</td>\n",
       "      <td>[B-prov, I-prov, E-prov, B-district, I-distric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[浙, 江, 省, 杭, 州, 市, 江, 干, 区, 白, 杨, 街, 道, 下, 沙, ...</td>\n",
       "      <td>[B-prov, I-prov, E-prov, B-city, I-city, E-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[秋, 菱, 路, 浙, 江, 兰, 溪, 金, 立, 达, 框, 业, 有, 限, 公, 司]</td>\n",
       "      <td>[B-road, I-road, E-road, B-poi, I-poi, I-poi, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 loc  \\\n",
       "0   [浙, 江, 杭, 州, 市, 江, 干, 区, 九, 堡, 镇, 三, 村, 村, 一, 区]   \n",
       "1  [浙, 江, 省, 温, 州, 市, 平, 阳, 县, 海, 西, 镇, 宋, 埠, 公, ...   \n",
       "2  [浙, 江, 省, 余, 姚, 市, 模, 具, 城, 金, 型, 路, 0, 0, 0, ...   \n",
       "3  [浙, 江, 省, 杭, 州, 市, 江, 干, 区, 白, 杨, 街, 道, 下, 沙, ...   \n",
       "4   [秋, 菱, 路, 浙, 江, 兰, 溪, 金, 立, 达, 框, 业, 有, 限, 公, 司]   \n",
       "\n",
       "                                               label  \n",
       "0  [B-prov, E-prov, B-city, I-city, E-city, B-dis...  \n",
       "1  [B-prov, I-prov, E-prov, B-city, I-city, E-cit...  \n",
       "2  [B-prov, I-prov, E-prov, B-district, I-distric...  \n",
       "3  [B-prov, I-prov, E-prov, B-city, I-city, E-cit...  \n",
       "4  [B-road, I-road, E-road, B-poi, I-poi, I-poi, ...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "171b0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/model/web/nlp02/data/dev.txt', 'r', encoding='utf-8') as f:\n",
    "    txt = f.read().split('\\n\\n')\n",
    "dev = pd.DataFrame([[t.split()[::2], t.split()[1::2]] for t in txt[:-1]])\n",
    "dev.columns = ['loc', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1927a92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[杭, 州, 五, 洲, 国, 际]</td>\n",
       "      <td>[B-city, E-city, B-poi, I-poi, I-poi, E-poi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[浙, 江, 省, 杭, 州, 市, 余, 杭, 乔, 司, 街, 道, 博, 卡, 路, ...</td>\n",
       "      <td>[B-prov, I-prov, E-prov, B-city, I-city, E-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[浙, 江, 诸, 暨, 市, 暨, 阳, 八, 一, 新, 村, 0, 0, 幢]</td>\n",
       "      <td>[B-prov, E-prov, B-district, I-district, E-dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[杭, 州, 市, 武, 林, 广, 场, 杭, 州, 大, 厦, 商, 城, A, 座, ...</td>\n",
       "      <td>[B-city, I-city, E-city, B-poi, I-poi, I-poi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[浙, 江, 省, 杭, 州, 市, 拱, 墅, 区, 登, 云, 路, 0, 0, 0, ...</td>\n",
       "      <td>[B-prov, I-prov, E-prov, B-city, I-city, E-cit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 loc  \\\n",
       "0                                 [杭, 州, 五, 洲, 国, 际]   \n",
       "1  [浙, 江, 省, 杭, 州, 市, 余, 杭, 乔, 司, 街, 道, 博, 卡, 路, ...   \n",
       "2         [浙, 江, 诸, 暨, 市, 暨, 阳, 八, 一, 新, 村, 0, 0, 幢]   \n",
       "3  [杭, 州, 市, 武, 林, 广, 场, 杭, 州, 大, 厦, 商, 城, A, 座, ...   \n",
       "4  [浙, 江, 省, 杭, 州, 市, 拱, 墅, 区, 登, 云, 路, 0, 0, 0, ...   \n",
       "\n",
       "                                               label  \n",
       "0       [B-city, E-city, B-poi, I-poi, I-poi, E-poi]  \n",
       "1  [B-prov, I-prov, E-prov, B-city, I-city, E-cit...  \n",
       "2  [B-prov, E-prov, B-district, I-district, E-dis...  \n",
       "3  [B-city, I-city, E-city, B-poi, I-poi, I-poi, ...  \n",
       "4  [B-prov, I-prov, E-prov, B-city, I-city, E-cit...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304e5aa",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a9ae93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载分词器\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('D:/model/web/nlp02/bert-base-chinese/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3b10ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['浙', '江', '省', '温', '州', '市', '平', '阳', '县', '海', '西', '镇', '宋', '埠', '公', '园', '南', '路', '0', '0', '0', '0', '号']\n"
     ]
    }
   ],
   "source": [
    "# 简单的示例\n",
    "test = '浙 江 省 温 州 市 平 阳 县 海 西 镇 宋 埠 公 园 南 路 0 0 0 0 号' \n",
    "# '浙江省温州市平阳县海西镇宋埠公园南路0000号' # ''.join(train[0][0])\n",
    "print(tokenizer.tokenize(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "76878b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3851, 3736, 4689, 3946, 2336, 2356, 2398, 7345, 1344, 3862, 6205, 7252, 2129, 1819, 1062, 1736, 1298, 6662, 121, 121, 121, 121, 1384, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a2befa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 浙 江 省 温 州 市 平 阳 县 海 西 镇 宋 埠 公 园 南 路 0 0 0 0 号 [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer(test)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77d8fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看标签数量\n",
    "from tkinter import _flatten\n",
    "total_labels = set(_flatten(list(train['label'])+ list(dev['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "523766ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab6f358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {j:i for i, j in enumerate(total_labels)}\n",
    "label2id['<UNK>'] = len(label2id)  # 验证集中存在训练集没有出现的标识\n",
    "id2label = dict(zip(label2id.values(), label2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25ca5ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-assist',\n",
       " 'B-cellno',\n",
       " 'B-city',\n",
       " 'B-community',\n",
       " 'B-devzone',\n",
       " 'B-distance',\n",
       " 'B-district',\n",
       " 'B-floorno',\n",
       " 'B-houseno',\n",
       " 'B-intersection',\n",
       " 'B-poi',\n",
       " 'B-prov',\n",
       " 'B-road',\n",
       " 'B-roadno',\n",
       " 'B-subpoi',\n",
       " 'B-town',\n",
       " 'B-village_group',\n",
       " 'E-assist',\n",
       " 'E-cellno',\n",
       " 'E-city',\n",
       " 'E-community',\n",
       " 'E-devzone',\n",
       " 'E-distance',\n",
       " 'E-district',\n",
       " 'E-floorno',\n",
       " 'E-houseno',\n",
       " 'E-intersection',\n",
       " 'E-poi',\n",
       " 'E-prov',\n",
       " 'E-road',\n",
       " 'E-roadno',\n",
       " 'E-subpoi',\n",
       " 'E-town',\n",
       " 'E-village_group',\n",
       " 'I-assist',\n",
       " 'I-cellno',\n",
       " 'I-city',\n",
       " 'I-community',\n",
       " 'I-devzone',\n",
       " 'I-distance',\n",
       " 'I-district',\n",
       " 'I-floorno',\n",
       " 'I-houseno',\n",
       " 'I-intersection',\n",
       " 'I-poi',\n",
       " 'I-prov',\n",
       " 'I-road',\n",
       " 'I-roadno',\n",
       " 'I-subpoi',\n",
       " 'I-town',\n",
       " 'I-village_group',\n",
       " 'O',\n",
       " 'S-assist',\n",
       " 'S-community',\n",
       " 'S-district',\n",
       " 'S-intersection',\n",
       " 'S-poi'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf27628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 56 57 1 {'S-poi'}\n"
     ]
    }
   ],
   "source": [
    "# 数据异常检测，验证集中存在一个训练集里没有的标签\n",
    "a = set(_flatten(list(train['label'])))\n",
    "b = set(_flatten(list(dev['label'])))\n",
    "print(len(a), len(b), len(a.union(b)), len(a.difference(b)), a.difference(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6e086",
   "metadata": {},
   "source": [
    "## 转化数据为id向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09aaf4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sentences, labels):\n",
    "    result = tokenizer(sentences, is_split_into_words=True)\n",
    "    result['labels'] = [[label2id.get(i, label2id['<UNK>']) for i in label] for label in labels]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37ff9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = get_data(list(train['loc']), train['label'])\n",
    "dev_inputs = get_data(list(dev['loc']), dev['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93084991",
   "metadata": {},
   "source": [
    "## 封装成数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c06466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_dict(train_inputs)\n",
    "dev_ds = Dataset.from_dict(dev_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15b52974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 8856\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a959195",
   "metadata": {},
   "source": [
    "# 模型加载\n",
    "\n",
    "最常见的token级别分类任务:\n",
    "\n",
    "- NER (Named-entity recognition 名词-实体识别) 分辨出文本中的名词和实体 (person人名, organization组织机构名, location地点名...).\n",
    "- POS (Part-of-speech tagging词性标注) 根据语法对token进行词性标注 (noun名词, verb动词, adjective形容词...)\n",
    "- Chunk (Chunking短语组块) 将同一个短语的tokens组块放在一起。\n",
    "\n",
    "对于以上任务，我们将展示如何使用简单的加载数据集，同时针对相应的任务使用transformer中的`Trainer`接口对模型进行微调。\n",
    "\n",
    "只要预训练的transformer模型最顶层有一个token分类的神经网络层（由于transformer的tokenizer新特性，还需要对应的预训练模型有fast tokenizer，参考[这个表](https://huggingface.co/transformers/index.html#bigtable)），那么本notebook理论上可以使用各种各样的transformer模型（[模型面板](https://huggingface.co/models)），解决任何token级别的分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e617b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" #需要是\"ner\", \"pos\" 或者 \"chunk\"\n",
    "model_checkpoint = \"D:/model/web/nlp02/bert-base-chinese/\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8edf9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "172e7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at D:/model/web/nlp02/bert-base-chinese/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(total_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f667fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=57, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c403b",
   "metadata": {},
   "source": [
    "## 模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "393a9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"D:/model/web/nlp02/test-{task}\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02ba4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification # 数据收集器\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer,\n",
    "    padding='longest', # 默认，按照最大的进行填充\n",
    "    label_pad_token_id=-100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "241911ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "# pip install seqeval\n",
    "metric = load(\"D:/model/web/nlp02/seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e8180a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 0.5,\n",
       " 'overall_recall': 0.5,\n",
       " 'overall_f1': 0.5,\n",
       " 'overall_accuracy': 0.8}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估指标例子\n",
    "predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "metric.compute(predictions=predictions, references=references)\n",
    "# 没去细究这些指标，但是总体的p,r,f1,acc还是知道的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fae8c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理预测结果\n",
    "# 1.选择预测分类最大概率的下标 2.将下标转化为label 3.忽略-100即padding所在的地方\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # 去掉特殊tokens的下标\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }  # 我们计算所有类别总的precision/recall/f1，所以会扔掉单个类别的precision/recall/f1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "517e1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微调\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=dev_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c2195be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2770' max='2770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2770/2770 08:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313058</td>\n",
       "      <td>0.820700</td>\n",
       "      <td>0.862864</td>\n",
       "      <td>0.841254</td>\n",
       "      <td>0.913028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>0.266648</td>\n",
       "      <td>0.865612</td>\n",
       "      <td>0.896339</td>\n",
       "      <td>0.880708</td>\n",
       "      <td>0.926529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>0.281541</td>\n",
       "      <td>0.869766</td>\n",
       "      <td>0.896946</td>\n",
       "      <td>0.883147</td>\n",
       "      <td>0.928035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.285197</td>\n",
       "      <td>0.880884</td>\n",
       "      <td>0.902710</td>\n",
       "      <td>0.891664</td>\n",
       "      <td>0.931501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.280257</td>\n",
       "      <td>0.884987</td>\n",
       "      <td>0.907362</td>\n",
       "      <td>0.896035</td>\n",
       "      <td>0.934816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.295663</td>\n",
       "      <td>0.885293</td>\n",
       "      <td>0.904632</td>\n",
       "      <td>0.894858</td>\n",
       "      <td>0.932917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.314006</td>\n",
       "      <td>0.882673</td>\n",
       "      <td>0.903115</td>\n",
       "      <td>0.892777</td>\n",
       "      <td>0.931531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.318059</td>\n",
       "      <td>0.886716</td>\n",
       "      <td>0.907969</td>\n",
       "      <td>0.897217</td>\n",
       "      <td>0.933309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.338088</td>\n",
       "      <td>0.888801</td>\n",
       "      <td>0.906958</td>\n",
       "      <td>0.897788</td>\n",
       "      <td>0.931802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.346988</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.903115</td>\n",
       "      <td>0.894879</td>\n",
       "      <td>0.930808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2770, training_loss=0.18008959319186985, metrics={'train_runtime': 494.2504, 'train_samples_per_second': 179.18, 'train_steps_per_second': 5.604, 'total_flos': 1550149964384112.0, 'train_loss': 0.18008959319186985, 'epoch': 10.0})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608cce2",
   "metadata": {},
   "source": [
    "# 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1438c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'D:/model/web/nlp02/test-ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "147e1039",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = AutoModelForTokenClassification.from_pretrained(model_path + '/checkpoint-2770/')\n",
    "tokenizer_test = AutoTokenizer.from_pretrained(model_path + '/checkpoint-2770/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee3a8054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3308, 7345, 1277, 2207, 1068, 1266, 7027, 121, 121, 121, 118, 121, 1384, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} 15\n"
     ]
    }
   ],
   "source": [
    "test_sample = '朝阳区小关北里000-0号'\n",
    "test_inputs = tokenizer_test(list(test_sample), is_split_into_words=True)\n",
    "print(test_inputs, len(test_inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c86c914b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 朝 阳 区 小 关 北 里 0 0 0 - 0 号 [SEP]'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_test.decode(test_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8579dd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 ['B-district', 'I-district', 'E-district', 'B-road', 'I-road', 'I-road', 'E-road', 'B-roadno', 'I-roadno', 'I-roadno', 'I-roadno', 'I-roadno', 'E-roadno', 'O', 'I-houseno']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.to('cpu')\n",
    "inputs = tokenizer_test(list(test_sample), is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model_test(**inputs).logits\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "predicted_token_class = [id2label[t.item()] for t in predictions[0]]\n",
    "print(len(predicted_token_class), predicted_token_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8bd39c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朝阳区小关北里000-0号 \n",
      " ['朝阳区', '小关北里', '000-0号'] \n",
      " ['district', 'road', 'roadno']\n"
     ]
    }
   ],
   "source": [
    "# 根据预测结果进行数据划分（predicted_token_class, test_sample）\n",
    "res = []\n",
    "lab = []\n",
    "for i, char in enumerate(test_sample):\n",
    "    sign = predicted_token_class[i]\n",
    "    if 'B' in sign:\n",
    "        word = char\n",
    "    elif 'E' in sign:\n",
    "        word = word + char\n",
    "        res.append(word)\n",
    "        lab.append(sign.split('-')[-1])\n",
    "    elif 'I' in sign:\n",
    "        word = word + char\n",
    "    elif 'S' in sign:\n",
    "        res.append(char)\n",
    "        lab.append(sign.split('-')[-1])\n",
    "        \n",
    "print(test_sample, '\\n',\n",
    "      res, '\\n',\n",
    "      lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088321a",
   "metadata": {},
   "source": [
    "## 具体化输出结果，把模型推理封装成函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bef5b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2zh = {'prov': '省级行政区', 'city': '地级行政区', 'district': '县级行政区', 'devzone': '广义的上的开发区', \n",
    " 'town': '乡级行政区', 'community': '村/社区', \n",
    " 'village_group': '组/队/社', 'road': '道路', 'roadno': '路号', 'poi': '兴趣点', 'subpoi': '子兴趣点', 'houseno': '楼栋号', \n",
    " 'cellno': '单元号', 'floorno': '楼层号', 'roomno': '房间号/户号', \n",
    " 'detail': 'poi内部的四层关系（house,cell,floor,room）没明确是哪一层，如 xx-xx-x-x，则整体标注 detail', \n",
    " 'assist': '普通辅助定位词', 'distance': '距离辅助定位词', \n",
    " 'intersection': '道路口，口、交叉口、道路（高速）出入口，一定与 road 同时出现，注意：小区出入口和车库出入口为 poi，“与”“和”两条路中间的修饰词为 redundant', \n",
    " 'redundant': '非地址元素', 'O': '以上标签未覆盖的情况', '<UNK>': '未知', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "03574bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spilt_location(test_sample):\n",
    "    inputs = tokenizer_test(list(test_sample), is_split_into_words=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model_test(**inputs).logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    predicted_token_class = [id2label[t.item()] for t in predictions[0]]\n",
    "#     print(predicted_token_class)\n",
    "    res = []\n",
    "    for i in range(len(test_sample)):\n",
    "        sign, char = predicted_token_class[i],  test_sample[i]\n",
    "        if 'B' in sign:\n",
    "            word = char\n",
    "        elif 'E' in sign:\n",
    "            word = word + char\n",
    "            res.append([lab2zh[sign.split('-')[-1]], word])\n",
    "        elif 'I' in sign:\n",
    "            word = word + char\n",
    "        elif 'S' in sign:\n",
    "            res.append([lab2zh[sign.split('-')[-1]], char])\n",
    "        elif '<UNK>' == sign:\n",
    "            pass\n",
    "        elif 'O' == sign:\n",
    "            if predicted_token_class[i-1] != 'O':\n",
    "                word = char\n",
    "            else:\n",
    "                word = word + char\n",
    "            if (predicted_token_class[i+1] != 'O') or (i == len(test_sample)-1):\n",
    "                res.append([lab2zh['O'], word])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b47536e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['地级行政区', '北京市'],\n",
       " ['县级行政区', '丰台区'],\n",
       " ['兴趣点', '草桥欣园'],\n",
       " ['子兴趣点', '四区'],\n",
       " ['楼栋号', '0号楼'],\n",
       " ['以上标签未覆盖的情况', '底商'],\n",
       " ['楼层号', '一层']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_spilt_location('北京市丰台区草桥欣园四区0号楼底商一层')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "616fa240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['地级行政区', '北京市'],\n",
       " ['县级行政区', '朝阳区'],\n",
       " ['道路', '广渠路'],\n",
       " ['兴趣点', '00号院'],\n",
       " ['楼层号', '甲000'],\n",
       " ['以上标签未覆盖的情况', '（'],\n",
       " ['兴趣点', '珠江帝景'],\n",
       " ['普通辅助定位词', '东北角'],\n",
       " ['以上标签未覆盖的情况', '商）']]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_spilt_location('北京市朝阳区广渠路00号院甲000（珠江帝景东北角底商）')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e60af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
