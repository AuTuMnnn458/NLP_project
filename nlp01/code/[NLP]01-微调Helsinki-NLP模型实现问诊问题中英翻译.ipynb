{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20879a30",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "\n",
    "\n",
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4b73a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>about how long have these symptoms been going on?</td>\n",
       "      <td>这些症状已持续多长时间？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and all chest pain should be treated this way ...</td>\n",
       "      <td>各种胸痛均应采取这种方法进行治疗，尤其要考虑年龄因素</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and along with a fever</td>\n",
       "      <td>并伴有发热症状</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and also needs to be checked your cholesterol ...</td>\n",
       "      <td>还需要检查一下胆固醇和血压</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and are you having a fever now?</td>\n",
       "      <td>您现在有发热吗？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and are you having any of the following sympto...</td>\n",
       "      <td>您的胸痛伴有以下任何症状吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and are you having a runny nose?</td>\n",
       "      <td>您有流鼻涕吗？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and are you having this chest pain now?</td>\n",
       "      <td>现在您有这种胸痛症状吗？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and besides do you have difficulty breathing</td>\n",
       "      <td>另外您有呼吸困难吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>and can you tell me what other symptoms are yo...</td>\n",
       "      <td>您能描述一下除此之外还有什么其他症状吗？</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  about how long have these symptoms been going on?   \n",
       "1  and all chest pain should be treated this way ...   \n",
       "2                             and along with a fever   \n",
       "3  and also needs to be checked your cholesterol ...   \n",
       "4                    and are you having a fever now?   \n",
       "5  and are you having any of the following sympto...   \n",
       "6                   and are you having a runny nose?   \n",
       "7            and are you having this chest pain now?   \n",
       "8       and besides do you have difficulty breathing   \n",
       "9  and can you tell me what other symptoms are yo...   \n",
       "\n",
       "                           zh  \n",
       "0                这些症状已持续多长时间？  \n",
       "1  各种胸痛均应采取这种方法进行治疗，尤其要考虑年龄因素  \n",
       "2                     并伴有发热症状  \n",
       "3               还需要检查一下胆固醇和血压  \n",
       "4                    您现在有发热吗？  \n",
       "5               您的胸痛伴有以下任何症状吗  \n",
       "6                     您有流鼻涕吗？  \n",
       "7                现在您有这种胸痛症状吗？  \n",
       "8                   另外您有呼吸困难吗  \n",
       "9        您能描述一下除此之外还有什么其他症状吗？  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = 'D:/model/web/nlp01'\n",
    "\n",
    "with open(data_path + '/corpus/tico-19.en-zh.en', 'r', encoding='utf-8') as f:\n",
    "    txt = f.read().split('\\n')\n",
    "    \n",
    "with open(data_path + '/corpus/tico-19.en-zh.zh', 'r', encoding='utf-8') as f:\n",
    "    txt2 = f.read().split('\\n')\n",
    "data = pd.DataFrame({'en': txt[:-1], 'zh': txt2[:-1]})\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c8940",
   "metadata": {},
   "source": [
    "## 数据划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca1306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3071, 2) (2456, 2) (615, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(data, test_size=0.2, random_state=0)\n",
    "print(data.shape, train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0344b72",
   "metadata": {},
   "source": [
    "## 加载tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4766995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712f18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = data_path + '/Helsinki-NLP--opus-mt-zh-en/'\n",
    "# 使用该tokenizer需要pip install sentencepiece\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2bedd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='D:/model/web/nlp01/Helsinki-NLP--opus-mt-zh-en/', vocab_size=65001, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t65000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec4fc2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5566, 26607, 2, 56, 30, 12, 95, 4509, 8233, 5, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('hello, this is a sentence.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a946855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [42473, 2, 12654, 10054, 863, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('您好，这是一个句子。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ddb50",
   "metadata": {},
   "source": [
    "## 把数据封装成datasets类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec5f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1981939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(train, tokenizer):\n",
    "    max_input_length = 128\n",
    "    max_target_length = 128\n",
    "\n",
    "    inputs = list(train['zh'].values)\n",
    "    targets = list(train['en'].values)\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']    \n",
    "    model_inputs['translation'] = [{'en':e, 'zh':c} for c, e in zip(inputs, targets)]\n",
    "    model_inputs = Dataset.from_dict(model_inputs)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9074a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_ds(train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3e76198",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = get_ds(val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4dc596f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels', 'translation'],\n",
      "    num_rows: 2456\n",
      "}) \n",
      " Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels', 'translation'],\n",
      "    num_rows: 615\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset, \n",
    "      '\\n',val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d79a32",
   "metadata": {},
   "source": [
    "# 预训练模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27769c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67c5e4f7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1041acb",
   "metadata": {},
   "source": [
    "## 使用预训练模型直接进行翻译\n",
    "不微调先看效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f5ac874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'企业对员工实施出行限制，取消会议，并鼓励员工居家办公。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.需要翻译的文本\n",
    "train.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9975adaa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    7,  1667,    63, 11747,   492,   854,  1699,  1323,     2,  4135,\n",
      "           336,     2,  5826, 11747,  6694,  1208,  8763,     9,     0]])\n"
     ]
    }
   ],
   "source": [
    "# 2.转换为token id\n",
    "inputs = tokenizer.encode(train.iloc[0,1], return_tensors='pt')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2acfa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65000, 39173, 10729,  2322,  4801,    18,   353,     2, 31137,   563,\n",
      "             6,  2441,   353,     8,   118,    46,  1430,     5,     0]])\n"
     ]
    }
   ],
   "source": [
    "# 3 模型预测\n",
    "decoder_inputs = model.generate(inputs)\n",
    "print(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee6788c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Enterprises impose travel restrictions on staff, cancel meetings and encourage staff to work at home.</s>\n"
     ]
    }
   ],
   "source": [
    "# 4 将预测结果转化为文本\n",
    "print(''.join(tokenizer.convert_ids_to_tokens(decoder_inputs[0])).replace('▁', ' ')) # 注意这里需要用空格替换下划线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd1575da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corporations imposed employee travel restrictions, cancelled conferences, and encouraged employees to work from home.\n"
     ]
    }
   ],
   "source": [
    "print(train.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1637c1b7",
   "metadata": {},
   "source": [
    "# 微调训练\n",
    "\n",
    "## 定义模型训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36eaaeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6ad8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    'D:/model/web/nlp01/ckpt',  # 模型checkpoint文件保存的路径\n",
    "    eval_strategy='epoch',  # 是否使用验证集进行模型评估。设置为epoch表示每个epcoh会做一次验证评估。\n",
    "    learning_rate=2e-5,  # 学习率\n",
    "    per_device_train_batch_size=batch_size,   # 训练过程中的批处理样本个数\n",
    "    per_device_eval_batch_size=batch_size,   # 验证过程中的批处理样本个数\n",
    "    weight_decay=0.01,  #不为零的情况下，要应用于除AdamW优化器中的所有偏置和LayerNorm权重之外的所有层的权重衰减。\n",
    "    save_total_limit=1,  # 模型保存的个数。至多保存3个模型\n",
    "    num_train_epochs=10,  # 训练次数\n",
    "    predict_with_generate=True,  # 是否使用生成的数据计算度量指标（如BLUE）\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4ecd9",
   "metadata": {},
   "source": [
    "## 定义数据收集器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "459b4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,  # 分词器\n",
    "    model,  # 预训练模型\n",
    "    label_pad_token_id=-100,   # padding对应的id，默认-100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa43434f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataCollatorForSeq2Seq in module transformers.data.data_collator object:\n",
      "\n",
      "class DataCollatorForSeq2Seq(builtins.object)\n",
      " |  DataCollatorForSeq2Seq(tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase, model: Optional[Any] = None, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, label_pad_token_id: int = -100, return_tensors: str = 'pt') -> None\n",
      " |  \n",
      " |  Data collator that will dynamically pad the inputs received, as well as the labels.\n",
      " |  \n",
      " |  Args:\n",
      " |      tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
      " |          The tokenizer used for encoding the data.\n",
      " |      model ([`PreTrainedModel`], *optional*):\n",
      " |          The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n",
      " |          prepare the *decoder_input_ids*\n",
      " |  \n",
      " |          This is useful when using *label_smoothing* to avoid calculating loss twice.\n",
      " |      padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
      " |          Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
      " |          among:\n",
      " |  \n",
      " |          - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |            sequence is provided).\n",
      " |          - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |            acceptable input length for the model if that argument is not provided.\n",
      " |          - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
      " |      max_length (`int`, *optional*):\n",
      " |          Maximum length of the returned list and optionally padding length (see above).\n",
      " |      pad_to_multiple_of (`int`, *optional*):\n",
      " |          If set will pad the sequence to a multiple of the provided value.\n",
      " |  \n",
      " |          This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
      " |          7.5 (Volta).\n",
      " |      label_pad_token_id (`int`, *optional*, defaults to -100):\n",
      " |          The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n",
      " |      return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n",
      " |          The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, features, return_tensors=None)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase, model: Optional[Any] = None, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, label_pad_token_id: int = -100, return_tensors: str = 'pt') -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'label_pad_token_id': <class 'int'>, 'max_length': ...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'label_pad_token_id': Field(name='label_pad_to...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('tokenizer', 'model', 'padding', 'max_length', 'pad_...\n",
      " |  \n",
      " |  label_pad_token_id = -100\n",
      " |  \n",
      " |  max_length = None\n",
      " |  \n",
      " |  model = None\n",
      " |  \n",
      " |  pad_to_multiple_of = None\n",
      " |  \n",
      " |  padding = True\n",
      " |  \n",
      " |  return_tensors = 'pt'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ef97e",
   "metadata": {},
   "source": [
    "## 定义评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf86d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "# pip install evaluate\n",
    "# pip install sacrebleu\n",
    "sacrebleu_path = r'D:\\model\\web\\nlp01\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--sacrebleu\\28676bf65b4f88b276df566e48e603732d0b4afd237603ebdf92acaacf5be99b\\sacrebleu.py'\n",
    "metric = load(sacrebleu_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "786b075a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"sacrebleu\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e82f8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ce78240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id\n",
    "def process_text(preds, labels):\n",
    "    preds = [i.strip() for i in preds]\n",
    "    labels = [i.strip() for i in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    decoder_inputs = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    labels = np.where(labels!= -100, labels, tokenizer.pad_token_id)\n",
    "    decoder_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    res = metric.compute(predictions=decoder_inputs, references=decoder_labels)\n",
    "    result = {'bleu': round(res['score'], 4)}\n",
    "    \n",
    "    # 添加评估指标：预测的平均长度\n",
    "    predict_len = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result['gen_len'] = round(np.mean(predict_len), 4)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0da7d3",
   "metadata": {},
   "source": [
    "## 创建trainer对象，进行模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24d451f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97d69f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mautumnnn\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\jupyter_notebook_doc\\个人项目\\wandb\\run-20250313_155549-u9u9mlga</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/autumnnn/huggingface/runs/u9u9mlga' target=\"_blank\">D:/model/web/nlp01/ckpt</a></strong> to <a href='https://wandb.ai/autumnnn/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/autumnnn/huggingface' target=\"_blank\">https://wandb.ai/autumnnn/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/autumnnn/huggingface/runs/u9u9mlga' target=\"_blank\">https://wandb.ai/autumnnn/huggingface/runs/u9u9mlga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3070' max='3070' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3070/3070 14:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.344449</td>\n",
       "      <td>30.480400</td>\n",
       "      <td>31.895900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>1.291110</td>\n",
       "      <td>31.950900</td>\n",
       "      <td>31.715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>1.276032</td>\n",
       "      <td>32.485900</td>\n",
       "      <td>31.471500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>1.275565</td>\n",
       "      <td>32.971800</td>\n",
       "      <td>31.790200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>1.277581</td>\n",
       "      <td>32.845300</td>\n",
       "      <td>31.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>1.296788</td>\n",
       "      <td>33.111100</td>\n",
       "      <td>31.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.637800</td>\n",
       "      <td>1.302871</td>\n",
       "      <td>33.428100</td>\n",
       "      <td>31.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.637800</td>\n",
       "      <td>1.312782</td>\n",
       "      <td>33.416800</td>\n",
       "      <td>31.923600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.555100</td>\n",
       "      <td>1.316131</td>\n",
       "      <td>33.451900</td>\n",
       "      <td>31.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>1.320113</td>\n",
       "      <td>33.545100</td>\n",
       "      <td>31.897600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3070, training_loss=0.8052966586929967, metrics={'train_runtime': 857.2399, 'train_samples_per_second': 28.65, 'train_steps_per_second': 3.581, 'total_flos': 402481642733568.0, 'train_loss': 0.8052966586929967, 'epoch': 10.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # 微调训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3830d70",
   "metadata": {},
   "source": [
    "## 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f3cbb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_test = AutoModelForSeq2SeqLM.from_pretrained('D:/model/web/nlp01/ckpt/checkpoint-3070/')\n",
    "tokenizer_test = AutoTokenizer.from_pretrained('D:/model/web/nlp01/ckpt/checkpoint-3070/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc0fd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 试一下pipeline\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e766c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8ccdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh2en = pipeline('translation_zh_to_en', \n",
    "                 model=model_test, \n",
    "                 tokenizer=tokenizer_test,\n",
    "                 device = 'cuda')\n",
    "# 不指定device会有下方提示\n",
    "# Hardware accelerator e.g. GPU is available in the environment, \n",
    "# but no `device` argument is passed to the `Pipeline` object. Model will be on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f6556f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = zh2en('您的胸痛伴有以下任何症状吗?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44eb808e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'do you have any of the following symptoms with your chest pain?'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40d5df0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do you have any of the following symptoms with your chest pain?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['translation_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0602d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 稍微封装一下\n",
    "def pipeline_output(text, model):\n",
    "    result = model(text)\n",
    "    return result[0]['translation_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4c4182d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i feel a pain in my lungs and can't sleep.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_output('我觉得我的肺部很痛，无法入睡。', zh2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039476fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
